{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNOsLo9yg8rhAKZRYj0yJnr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HenryBlackie/TikTok-Research-API/blob/main/TikTok_Research_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TikTok Research API\n",
        "- Add proper error handling\n",
        "- Add code to detect and report expired access token\n",
        "- Add code to handle an exceeded quota limit"
      ],
      "metadata": {
        "id": "B1mBvd3wlXKU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tf2bYiW_lRoN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title TikTok Client Setup {run: \"auto\"}\n",
        "\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def gen_access_token():\n",
        "    client_key = \"\" # @param {type:\"string\"}\n",
        "    client_secret = \"\" # @param {type:\"string\"}\n",
        "\n",
        "    # Configure request data\n",
        "    url = \"https://open.tiktokapis.com/v2/oauth/token/\"\n",
        "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
        "    data = {\"client_key\": client_key,\n",
        "            \"client_secret\": client_secret,\n",
        "            \"grant_type\": \"client_credentials\"}\n",
        "\n",
        "    # Get OAuth token from TikTok API\n",
        "    oauth_token = requests.post(url=url, headers=headers, data=data).json()\n",
        "\n",
        "    # Print token generation status\n",
        "    if \"access_token\" in oauth_token:\n",
        "        print(\"Access token successfully generated.\")\n",
        "    else:\n",
        "        print(\"Failed to generate access token\")\n",
        "        print(f\"{oauth_token['error']}: {oauth_token['error_description']}\")\n",
        "\n",
        "    return oauth_token['access_token']\n",
        "\n",
        "def query_video_comments(video_id, client_access_token=\"\"):\n",
        "    # Generate access token if not provided\n",
        "    if not client_access_token:\n",
        "        client_access_token = gen_access_token()\n",
        "\n",
        "    api_url = \"https://open.tiktokapis.com/v2/research/video/comment/list/\"\n",
        "    fields = \"id,video_id,text,like_count,reply_count,parent_comment_id,create_time\"\n",
        "    headers = {\"Authorization\": f\"Bearer {client_access_token}\",\n",
        "               \"Content-Type\": \"application/json\"}\n",
        "    data = {\"video_id\": video_id,\n",
        "            \"max_count\": 100,\n",
        "            \"cursor\": 0}\n",
        "\n",
        "    # Keep sending queries until TikTok indicates there are no more comments\n",
        "    comment_objects = []\n",
        "    retry_count = 0\n",
        "    while data['cursor'] < 1000 and retry_count < 5:\n",
        "        decode_error = True\n",
        "        while decode_error:\n",
        "            print(f\"Requesting comments {data['cursor']} to {data['cursor'] + data['max_count']}.\")\n",
        "            # Get data from TikTok API\n",
        "            response = requests.post(f\"{api_url}?fields={fields}\", json=data, headers=headers)\n",
        "\n",
        "            # Save response to log\n",
        "            log_response(str(response.text))\n",
        "\n",
        "            # Convert content to dict\n",
        "            try:\n",
        "                response = response.json()\n",
        "                decode_error = False\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                time.sleep(60)\n",
        "                decode_error = True\n",
        "\n",
        "        # Break out of while loop if an error is returned\n",
        "        if response[\"error\"][\"code\"] != \"ok\":\n",
        "            print(response[\"error\"][\"code\"] + \" : \" + response[\"error\"][\"message\"])\n",
        "\n",
        "            # Retry on timeout error\n",
        "            if response[\"error\"][\"code\"] == \"timeout\":\n",
        "                print(\"Retrying in 30 seconds.\")\n",
        "                time.sleep(30)\n",
        "                retry_count += 1\n",
        "                continue\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        # Save comment objects\n",
        "        if len(response[\"data\"][\"comments\"]) > 0:\n",
        "            comment_objects = comment_objects + response[\"data\"][\"comments\"]\n",
        "\n",
        "        # Break if there are no more comments\n",
        "        if not response[\"data\"][\"has_more\"]:\n",
        "            break\n",
        "\n",
        "        # Update cursor\n",
        "        data[\"cursor\"] = response[\"data\"][\"cursor\"]\n",
        "\n",
        "        # Wait before sending next query\n",
        "        time.sleep(1)\n",
        "\n",
        "    print(f\"Returning {len(comment_objects)} comments.\")\n",
        "    return comment_objects\n",
        "\n",
        "def query_all_user_videos(username, search_start_date, search_end_date):\n",
        "    # Generate client access token\n",
        "    client_access_token = gen_access_token()\n",
        "\n",
        "    # Convert search dates to datetime objects\n",
        "    search_start_date = datetime.strptime(search_start_date, \"%Y-%m-%d\").date()\n",
        "    search_end_date = datetime.strptime(search_end_date, \"%Y-%m-%d\").date()\n",
        "\n",
        "    # Configure API parameters\n",
        "    api_url = \"https://open.tiktokapis.com/v2/research/video/query/\"\n",
        "    fields = \"id,video_description,create_time, region_code,share_count,view_count,like_count,comment_count, music_id,hashtag_names,username,effect_ids,playlist_id,voice_to_text\"\n",
        "    headers = {\"Authorization\": f\"Bearer {client_access_token}\",\n",
        "               \"Content-Type\": \"application/json\"}\n",
        "    data = {\"query\": {\"and\": [{\"field_name\": \"username\",\n",
        "                               \"operation\": \"EQ\",\n",
        "                               \"field_values\": [username]}]},\n",
        "            \"max_count\": 100,\n",
        "            \"cursor\": 0,\n",
        "            \"start_date\": search_start_date,\n",
        "            \"end_date\": None}\n",
        "    date_delta = {\"offset\": 0,\n",
        "                  \"interval\": 30}\n",
        "\n",
        "    print(f\"Username: {username}\\nTime Frame: {search_start_date} to {search_end_date}\")\n",
        "\n",
        "    video_objects = []\n",
        "    # Query video IDs between date boundaries\n",
        "    while data[\"start_date\"] + timedelta(days=date_delta[\"interval\"]) <= search_end_date:\n",
        "        # Calculate date boundaries for search\n",
        "        data[\"start_date\"] = search_start_date + timedelta(days=date_delta[\"offset\"])\n",
        "        data[\"end_date\"] = data[\"start_date\"] + timedelta(days=date_delta[\"interval\"])\n",
        "        if data[\"end_date\"] >= search_end_date:\n",
        "            data[\"end_date\"] = search_end_date\n",
        "\n",
        "        print(f\"Requesting videos between {data['start_date']} and {data['end_date']}.\")\n",
        "\n",
        "        # Convert dates to correct format YYYYMMDD\n",
        "        data[\"start_date\"] = data[\"start_date\"].strftime(\"%Y%m%d\")\n",
        "        data[\"end_date\"] = data[\"end_date\"].strftime(\"%Y%m%d\")\n",
        "\n",
        "        # Send request to TikTok\n",
        "        response = requests.post(f\"{api_url}?fields={fields}\", json=data, headers=headers).json()\n",
        "        if response[\"error\"][\"code\"] != \"ok\":\n",
        "            print(response[\"error\"][\"code\"] + \" : \" + response[\"error\"][\"message\"])\n",
        "\n",
        "        # Save video objects\n",
        "        if len(response[\"data\"][\"videos\"]) > 0:\n",
        "            video_objects = video_objects + response[\"data\"][\"videos\"]\n",
        "\n",
        "        # Keep repeating search until has_more flag is False\n",
        "        while response[\"data\"][\"has_more\"]:\n",
        "            # Update cursor\n",
        "            data[\"cursor\"] = response[\"data\"][\"cursor\"]\n",
        "            print(f\"Continuing request from index {data['cursor']}\")\n",
        "\n",
        "            # Send request to TikTok\n",
        "            response = requests.post(f\"{api_url}?fields={fields}\", json=data, headers=headers).json()\n",
        "            if response[\"error\"][\"code\"] != \"ok\":\n",
        "                print(response[\"error\"][\"code\"] + \" : \" + response[\"error\"][\"message\"])\n",
        "\n",
        "            # Save video objects\n",
        "            if len(response[\"data\"][\"videos\"]) > 0:\n",
        "                video_objects = video_objects + response[\"data\"][\"videos\"]\n",
        "\n",
        "        # Restore dates to original format YYYY-MM-DD\n",
        "        data[\"start_date\"] = datetime.strptime(data[\"start_date\"], \"%Y%m%d\").date()\n",
        "        data[\"end_date\"] = datetime.strptime(data[\"end_date\"], \"%Y%m%d\").date()\n",
        "\n",
        "        # Reset cursor\n",
        "        data[\"cursor\"] = 0\n",
        "\n",
        "        # Update offset\n",
        "        date_delta[\"offset\"] += date_delta[\"interval\"]\n",
        "\n",
        "        # Wait before sending next query\n",
        "        #time.sleep(1)\n",
        "\n",
        "    # Save videos data\n",
        "    save_data(pd.DataFrame.from_dict(video_objects), username, \"videos\", \"video_objects\")\n",
        "\n",
        "    # Iterate through videos and request comments for each\n",
        "    for idx, video_data in enumerate(video_objects):\n",
        "        print(f\"Requesting data for video {idx+1} of {len(video_objects)}.\")\n",
        "        comment_objects = query_video_comments(video_data[\"id\"], client_access_token)\n",
        "\n",
        "        save_data(pd.DataFrame.from_dict(comment_objects), username, video_data[\"id\"], \"comment_objects\")\n",
        "\n",
        "def extract_url_data(url):\n",
        "    # Define regex pattern\n",
        "    pattern = r\"@([a-zA-Z0-9_\\.]+).*?/?(?:video|photo)?/?(\\d+)?|\\@([a-zA-Z0-9_\\.]+)$\"\n",
        "\n",
        "    # Use re.search to find the pattern in the URL\n",
        "    match = re.search(pattern, url)\n",
        "\n",
        "    # Return matches if found\n",
        "    if match:\n",
        "        username = match.group(1) or match.group(3)\n",
        "        video_id = match.group(2) if match.group(2) else None\n",
        "        return username, video_id\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def save_data(df, username, filename, data_type):\n",
        "    if \"g_drive_dir\" in globals():\n",
        "        save_dir = g_drive_dir\n",
        "    else:\n",
        "        save_dir = \"/content/\"\n",
        "\n",
        "    # Save data\n",
        "    if data_type == \"video_objects\":\n",
        "        os.makedirs(os.path.join(save_dir, username), exist_ok=True)\n",
        "        filepath = os.path.join(save_dir, username, f\"{filename}.xlsx\")\n",
        "        df.to_excel(filepath)\n",
        "        print(f\"Saved data to {filepath}.\")\n",
        "    elif data_type == \"comment_objects\":\n",
        "        os.makedirs(os.path.join(save_dir, username, \"comments\"), exist_ok=True)\n",
        "        filepath = os.path.join(save_dir, username, \"comments\", f\"{filename}.xlsx\")\n",
        "        df.to_excel(filepath)\n",
        "        print(f\"Saved data to {filepath}.\")\n",
        "\n",
        "def log_response(content):\n",
        "    if \"g_drive_dir\" in globals():\n",
        "        save_dir = g_drive_dir\n",
        "    else:\n",
        "        save_dir = \"/content/\"\n",
        "\n",
        "    with open(os.path.join(save_dir, \"responses.log\"), \"a+\") as f:\n",
        "        f.write(f\"{content}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup Google Drive\n",
        "# @markdown Run this to setup a connection to Google Drive. Otherwise all output data will be lost upon disconnection.\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "output_dir = \"Colab Data/tiktok-research-api\" # @param {type:\"string\"}\n",
        "global g_drive_dir\n",
        "g_drive_dir = os.path.join(\"/content/drive/MyDrive/\", output_dir)\n",
        "print(f\"Setup output directory as {g_drive_dir}.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UPclB3Z44PM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Get Video Comments\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "#video_id = 7222436435448646955 # @param {type:\"integer\"}\n",
        "url = \"\" # @param {type:\"string\"}\n",
        "username, video_id = extract_url_data(url)\n",
        "\n",
        "comment_objects = query_video_comments(video_id)\n",
        "\n",
        "if len(comment_objects) > 0:\n",
        "    # Build comments into DataFrame\n",
        "    df = pd.DataFrame.from_dict(comment_objects)\n",
        "    os.makedirs(\"/content/video_comments/\")\n",
        "    df.to_excel(f\"/content/video_comments/{video_id}.xlsx\")\n",
        "    print(f\"Saved {len(df)} comments to {video_id}.xlsx\")\n",
        "else:\n",
        "    print(f\"No comments were saved for video {video_id}.\")"
      ],
      "metadata": {
        "id": "tMPaF8Kj0BOm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Get User Data\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from google.colab import output\n",
        "output.no_vertical_scroll()\n",
        "\n",
        "url = \"\" # @param {type:\"string\"}\n",
        "username, video_id = extract_url_data(url)\n",
        "\n",
        "search_start_date = None # @param {type:\"date\"}\n",
        "search_end_date = None # @param {type:\"date\"}\n",
        "# @markdown It is highly recommended to set search start and end dates. If left blank, the script will search from September 2016 to the current date. This will likely take a long time and is a wasteful use of your API quota.\n",
        "\n",
        "# Set start/end dates to default values if they were not provided\n",
        "# The default start date is 2016-09-20 - Douyin initial release\n",
        "# The default end date is the current date\n",
        "if search_start_date == None:\n",
        "    search_start_date = \"2016-09-20\"\n",
        "if search_end_date == None:\n",
        "    search_end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "query_all_user_videos(username, search_start_date, search_end_date)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "aKrEYTW4ujJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debugging"
      ],
      "metadata": {
        "id": "zVQ7H4Uqj0ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title URL Extraction Test {run: \"auto\"}\n",
        "# @markdown This codeblock can be used to verify the data being extracted from any provided URLs. This is only useful for debugging.\n",
        "\n",
        "url = \"\" # @param {type:\"string\"}\n",
        "username, video_id = extract_url_data(url)\n",
        "\n",
        "print(f\"Username: {username}\")\n",
        "print(f\"Video ID: {video_id}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yGMPGt5QTm_V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}